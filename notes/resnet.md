## ResNet Architecture

### Related Works
- **Attention Mechanisms**, shows evidence from human perception highlights the importance of attention mechanisms, that help the process of top down information to guid bottom up feedforward process.
- **RNN** and **LSTM** uses attention to gather top information to decide where to attend the next feature learning steps.

### Architecture
- The network is constructed by stacking multiple attention modules. Attention module that is used is divided into two branches which is mask branch and trunk branch. Trunk Branch performs 
